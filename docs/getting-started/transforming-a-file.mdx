import Authentication from '../_partials/_authentication.mdx';

# Transforming a file

A detailed description and information on [Transforms](../transform) you can review the specific page [here](../transform).

<Authentication/>

## Listing files

If no files have been uploaded to the [File Vault](../filevaults) yet, please refer to the [uploading a file getting started guide](./uploading-a-file-to-a-filevault).

If files are available to transform you will need the file IDs. To obtain the file IDs, run:

```bash
curl --location --request GET 'https://api.integration.k8s.is/api/v2/files/' \
--header 'Authorization: $INFOSUM_API_KEY'
```

This will give you a list of available files within the File Vault and the associated meta data.

## Transformation scripts

To modify, otherwise known as transform, a file you will need a `bloblang` script to do this.

### Writing a transformation script

Let's assume that the data that needs transforming is in the form of a CSV, like so:

|userID|cat_action|cat_romance|cat_drama|cat_scifi|
|-|-|-|-|-|
|U0001|1|1|1|1|
|U0005|0|1|1|0|
|U9999|1|0|0|1|

What would we have to do if we wanted to transform those boolean columns into a simpler format of a `userID` with just the associated `cat_*`?
The following bloblang script does this. By declaring the `input` file and how to read it, the `bloblang` itself as a transformation pipeline, and the `output` for the format to return to the File Vault.

```yaml
input:
  concatenate_files:
    sort_columns: [ "userID" ]
    shard_total: 0
    shard_index: 0
    files:
      - pattern: "*.csv"
        format:
        type: csv
        csv:
          delimiter: ","
          headers: true

pipeline:
  processors:
    - bloblang: |
        # Get the root (row)
        root = this

        # Set a map of key-values as sorted by the key (column name)
        root.kvs = this.key_values().sort_by(pair -> pair.key)
        
        # Set the 'category' parameter of the root to a map where:
        # For each element in the sorted key-values:
          # If the value is set to \"1\" AND the key (column header) contains \"cat\"
            # Then set the value to be the key (column header)
          # Else
            # Throw away the element
        root.category = root.kvs.map_each(kv -> if kv.value == "1" && kv.key.contains("cat") {kv.key} else {deleted()})
        # Example:
          # In:  {"kvs":{"cat_1":"1","cat_2":"0","cat_3":"1","something_else":"1"}}
          # Out: {"category":["cat_1", "cat_3"}}
          
output:
  store_file:
    name: reverse_encoded.sdf
    schema:
      - { name: userID,      type: STRING }
      - { name: category,       type: STRING_ARRAY}
```

For more details on `bloblang`, please refer to our [bloblang documentation](../transform#bloblang).

### Checking the script for errors

Once you have a `bloblang` script, you can validate it before executing it on large amounts of data.

To do this you will need to encode your `bloblang` script into the `script` element like the following API request:

```bash
curl --location --request POST 'https://api.integration.k8s.is/api/v2/transform/scripts/validate' \
--header 'Authorization: $INFOSUM_API_KEY' \
--header 'Content-Type: application/json' \
--data-raw '{
 "script":"input:\n  concatenate_files:\n    sort_columns: [ \"userID\" ]\n    shard_total: 0\n    shard_index: 0\n    files:\n      - pattern: \"*.csv\"\n        format:\n          type: csv\n          csv:\n            delimiter: \",\"\n            headers: true\n\npipeline:\n  processors:\n    - bloblang: |\n        # Get the root (row)\n        root = this\n\n        # Set a map of key-values as sorted by the key (column name)\n        root.kvs = this.key_values().sort_by(pair -> pair.key)\n\n        # Set the '\''category'\'' parameter of the root to a map where:\n        # For each element in the sorted key-values:\n          # If the value is set to \"1\" AND the key (column header) contains \"cat\"\n            # Then set the value to be the key (column header)\n          # Else\n            # Throw away the element\n        root.category = root.kvs.map_each(kv -> if kv.value == \"1\" && kv.key.contains(\"cat\") {kv.key} else {deleted()}) \n        # Example:\n          # In:  {\"kvs\":{\"cat_1\":\"1\",\"cat_2\":\"0\",\"cat_3\":\"1\",\"something_else\":\"1\"}}\n          # Out: {\"category\":[\"cat_1\", \"cat_3\"}}\n\noutput: \n  store_file:\n    name: reverse_encoded.sdf\n    schema:\n      - { name: userID,      type: STRING }\n      - { name: category,       type: STRING_ARRAY}\n"
 }'
```

### Running a transformation

If the `bloblang` script has passed validation you can now execute it against your files, you can run either a `scaled` or `static` transformation task.

The information required by `static` transformation endpoint is:

- The `script` which is the encoded `bloblang` you wish to execute
- A `file_vault_id` which references where the `input` files in the `script` are housed
- The `compute_instance_id` of where the `script` will be run

```bash
curl --location --request POST 'https://api.integration.k8s.is/api/v2/transform/static-tasks' \
--header 'Authorization: $INFOSUM_API_KEY' \
--header 'Content-Type: application/json' \
--data-raw '{
  "script": "input:\n  concatenate_files:\n    sort_columns: [ \"userID\" ]\n    shard_total: 0\n    shard_index: 0\n    files:\n      - pattern: \"*.csv\"\n        format:\n          type: csv\n          csv:\n            delimiter: \",\"\n            headers: true\n\npipeline:\n  processors:\n    - bloblang: |\n        root = this\n        root.kvs = this.key_values().sort_by(pair -> pair.key)\n        root.category = root.kvs.map_each(kv -> if kv.value == \"1\" && kv.key.contains(\"cat\") {kv.key} else {deleted()}) \n    \noutput: \n  store_file:\n    name: demo_output.sdf\n    schema:\n      - { name: userID,      type: STRING }\n      - { name: category,       type: STRING_ARRAY}\n",
  "file_vault_id": "",
  "compute_instance_id": ""
}'
```